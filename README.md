# Unlearnable Examples

This repository contains the code that is used in the experiments described in my bachelor thesis titled: Exploring Unlearnable Examples. 

## Abstract 

The large amount of publicly accessible image data has been key to the success of machine learning. However, the publicly accessible data also raises
privacy concerns about unauthorized data exploitation. To protect personal images from unauthorized neural network training, error-minimizing
noise has been proposed. The error-minimizing noise can be added to personal images making them unusable for training neural networks. Therefore,
these images are also referred to as unlearnable examples. This thesis aims
to improve the performance of deep neural networks trained on data with
added error-minimizing noise, and thereby give a direction on how to improve the noise. The research explores the strength of error-minimizing noise
by focusing on the following two aspects: its resistance against adversarial
training, and its resistance against data augmentation. We show that the
effects of the noise are highly dependent on the presence of color by conducting experiments on grayscale images. We demonstrate that generating
error-minimizing noise on grayscale image does not improve its resistance
against grayscale transformations. Furthermore, we verify that the effect of
error-minimizing noise can be compensated by using adversarial training.

First supervisor:\
MSc. Zhuoran Liu

Second supervisor:\
MSc. Alex Kolmus

First assessor:\
Prof. Martha Larson

Second assessor:\
Prof. Tom Heskes

